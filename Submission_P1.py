# -*- coding: utf-8 -*-
"""Copy of Data Understanding Dicoding - Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iKn4u_DQe6DaNH-_7gARUta_B_n-GLF

# Nama : Farras Nur Haidar Ramadhan
# Email : farrasnurhaidar04@gmail.com
# username Dicoding : farrasnhr

# Library yang digunakan

Menyiapkan beberapa library guna menunjang pengerjaan proyek. Berikut library yang akan digunakan pada proyek ini:
"""

# Library dasar untuk manipulasi data dan visualisasi
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Library untuk preprocessing
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Library untuk pembagian data
from sklearn.model_selection import train_test_split

# Library untuk algoritma machine learning
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# Library untuk evaluasi model
from sklearn.metrics import accuracy_score, confusion_matrix

# Library untuk unngah berkas
from google.colab import files

"""# Data Loading

Proses ini dilakukan pengambilan dataset dari Kaggle dari *setup* *username* dan *key* dalam bentuk `json`, kemudian melakukan pengunduhan *resource*, dan data yang telah diunduh dalam bentuk zip akan dilakukan ekstraksi.

## Mengunduh resource

Mengunggah berkas berisi username dan key untuk mendownload resource berupa dataset di Kaggle
"""

files.upload()

"""Melakukan instalasi Kaggle kemudian mengunduh resource dan ekstraksi resource"""

!pip install kaggle

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

# Mengunduh dataset Fruits 360
!kaggle datasets download -d mingyuouyang/chicago-crime-dataset-2018-to-2021

# Mengekstrak dataset
!unzip chicago-crime-dataset-2018-to-2021.zip -d crime

"""# Data Understanding

Dataset yang digunakan untuk proyek ini adalah [Chicago Crime Dataset 2018 to 2021](https://www.kaggle.com/datasets/mingyuouyang/chicago-crime-dataset-2018-to-2021?select=Crimes_-_2018.csv) yang diambil dari laman Kaggle. Dataset tersebut memiliki 4 file dengan format csv berukuran 226.34 MB

Melihat jumlah dari masing-masing berkas csv
"""

df = pd.read_csv("crime/Crimes_-_2018.csv")

df.shape

"""1. dataset untuk tahum 2018 berisi sebanyak 268602 baris dan 22 kolom."""

df = pd.read_csv("crime/Crimes_-_2019.csv")

df.shape

"""2. dataset untuk tahum 2019 berisi sebanyak 261019 baris dan 22 kolom."""

df = pd.read_csv("crime/Crimes_-_2020.csv")

df.shape

"""3. dataset untuk tahum 2020 berisi sebanyak 211716 baris dan 22 kolom."""

df = pd.read_csv("crime/Crimes_-_2021.csv")

df.shape

"""4. dataset untuk tahum 2021 berisi sebanyak 207087 baris dan 22 kolom.

Membuat folder dan menggabungkan 4 file csv menjadi 1 file csv
"""

import pandas as pd
import os

# Path ke folder tempat file diekstrak
folder_path = 'crime'

# Buat list untuk menyimpan setiap DataFrame
dfs = []

# Loop melalui file CSV di folder dan gabungkan
for file in os.listdir(folder_path):
    if file.endswith('.csv'):
        file_path = os.path.join(folder_path, file)
        # Membaca setiap file CSV
        df = pd.read_csv(file_path)
        dfs.append(df)

# Menggabungkan semua DataFrame menjadi satu
df = pd.concat(dfs, ignore_index=True)

print(f"Total baris dalam dataset: {df.shape[0]} dan {df.shape[1]} kolom")

"""Setelah menggabungkan 4 berkas csv menjadi 1 berkas csv, total data menjadi sebanyak 948424 baris dan 22 kolom

## Cek Duplikasi

Melakukan pengecekan pada dataset apakah memiliki data duplikat.
"""

# Mengecek jumlah baris duplikat
jumlah_duplikat = df.duplicated().sum()
print(f"Jumlah baris duplikat: {jumlah_duplikat}")

# Menampilkan baris-baris yang duplikat (jika ingin melihat detailnya)
df_duplikasi = df[df.duplicated()]
print("Baris duplikat:\n", df_duplikasi)

"""dari *ouput* diatas tidak terdapat duplikasi data.

## Cek Missing Values

Melakukan pengecekan apakah semua kolom nilainya sama
"""

df.info()

"""Melihat ada beberapa kolom yang nilainya tidak sama, dilakukan pengecekan missing values"""

# Mengecek jumlah missing values di setiap kolom
missing_values = df.isnull().sum()
print("Jumlah missing values di setiap kolom:\n", missing_values)

# Menampilkan total missing values dalam dataset
total_missing = df.isnull().sum().sum()
print(f"\nTotal missing values dalam dataset: {total_missing}")

"""Terdapat total 76284 missing values atau yang tidak terisi dari beberapa fitur.

## Cek Outliers

Melihat apakah terdapat data yang dianggap *outliers* dengan metode IQR
"""

# Memilih kolom numerik saja
numerik_df = df.select_dtypes(include=['number'])

# Menghitung IQR dan mendeteksi outliers untuk setiap kolom numerik
Q1 = numerik_df.quantile(0.25)
Q3 = numerik_df.quantile(0.75)
IQR = Q3 - Q1

# Menentukan outliers di luar rentang (Q1 - 1.5*IQR, Q3 + 1.5*IQR)
outliers = ((numerik_df < (Q1 - 1.5 * IQR)) | (numerik_df > (Q3 + 1.5 * IQR)))

# Menampilkan jumlah outliers per kolom
print("Jumlah outliers per kolom:\n", outliers.sum())

"""Output diatas menunjukkan bahwa terdapat outliers pada beberapa fitur.

## EDA

### Deskripsi Variabel

Melakukan visual pada dataset
"""

df.info()

"""Dari informasi dataset diatas, dapat diuraikan fitur-fiturnya sebagai berikut:
1. `ID` berisi nomor unik setiap kejadian,
2. `Case Number` berisi informasi nomor kasus kejadian,
3. `Date` berisi tanggal terjadinya kejahatan di Chicago,
4. `Block` berisi alamat atau lokasi tempat kejadian,
5. `IUCR` berisi kode klasifikasi Illinois Uniform Crime Reporting (IUCR) yang mewakili jenis kejahatan,
6. `Primary Type` berisi informasi jenis kejahatan yang terjadi di Chicago,
7. `Description` berisi deskripsi lebih rinci tentang kejahatan dalam kategori utama,
8. `Location Description` berisi informasi terkait tempat kejadian,
9. `Arrest` berisi informasi apakah ada penangkapan pelaku kejadian,
10. `Domestic` berisi tentang apakah kejahatan tersebut berkaitan dengan kekerasan rumah tangga,
11. `Beat` berisi informasi yang menunjukkan wilayah patroli kepolisian tempat kejadian kejahatan itu terjadi,
12. `Distric` berisi tentang distrik kepolisian di mana kejadian dilaporkan,
13. `Ward` berisi dimana wilayah administratif tempat kejadian terjadi,
14. `Community Area` berisi kode wilayah komunitas atau area geografis tempat kejahatan itu terjadi,
15. `FBI Code` berisi kode standar dari FBI yang mengklasifikasikan jenis kejahatan,
16. `X Coordinate` berisi koordinat X untuk menunjukkan lokasi di peta,
17. `Y Coordinate` berisi koordinat Y untuk menunjukkan lokasi di peta,
18. `Year` berisi tahun kejadian terjadi,
19. `Update On` berisi data tersebut diperbaharui,
20. `Latitude` berisi tentang informasi koordinasi lokasi berupa garis lintang,
21. `Longitude` berisi tentang informasi koordinasi lokasi berupa garis bujur,
22. `Location` berisi gabungan fitur `Latitude` dan `Longitude`.

Terdapat 22 fitur pada dataset dengan berbagai variasi tipe data diantaranya: `bool` sebanyak dua fitur, `float64` sebanyak 6 fitur, `int64` sebanyak 4 fitur, dan `object` sebanyak 10 fitur. Dan berikut tampilan 5 baris teratas setiap fitur:
"""

df.head()

"""Pada proyek ini akan berfokus pada klasifikasi area tingkat kejahatan, maka dari itu dilakukan pemilihan fitur agar dapat menyederhanakan model. Dipilihlah beberapa diantaranya `Primary Type`, `Date`, `Location Description`, `Arrest`, `Domestic`, `Beat`, `District` `Latitude`, `Longitude`, dan `Community Area`"""

# Fitur yang akan digunakan untuk analisis atau modeling
keep = ['Primary Type', 'Date', 'Location Description', 'Arrest', 'Domestic',
                    'Beat', 'District', 'Latitude', 'Longitude', 'Community Area']

# Memilih hanya fitur yang relevan
df = df[keep]

df.head()

"""### Ekstraksi

Untuk kebutuhan EDA, fitur `Date` akan diekstrak menjadi bertambah fitur `hour` dan `Day of Week`

Sebelum diekstraksi, fitur `date` akan diubah dari tipe data `object` menjadi `datetime64`
"""

# Melihat 5 barispertama dari kolom Date
print('================ Sebelum ================\n')
print(df['Date'].head())

df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %I:%M:%S %p')

print('\n\n================ Sesudah ================\n')
print(df['Date'].head())

"""Hasil diatas menunjukkan format date sebelum dan sesudah diubah tipe datanya, kemudian dilakukan proses ekstraksi untuk mendapatkan fitur `Hour` dan `Day of Week`."""

# Konversi kolom Date menjadi format datetime
df['Date'] = pd.to_datetime(df['Date'])

# Ekstraksi jam dan hari dalam seminggu dari kolom Date
df['Hour'] = df['Date'].dt.hour
df['Day of Week'] = df['Date'].dt.dayofweek

# Menampilkan beberapa baris pertama setelah menambahkan kolom Hour dan Day of Week
print(df[['Date', 'Hour', 'Day of Week']].head())

"""Melihat kembali info dataset."""

# Informasi tentang DataFrame
df.info()

"""Dari hasil diatas dapat dilihat bahwa didalam dataset berisi 948424 baris dari 11 kolom. Dari 11 kolom memiliki variasi tipe data yang bervariasi yaitu terdapat tipe data `bool` dua kolom, `datetime64` satu kolom, `float64` 3 kolom, `int32` dua kolom, `int64` dua kolom, `object` dua kolom. kolom berisi informasi / fitur baru yakni:
1. `Hour` berisi informasi jam di mana kejadian dilaporkan terjadi, dalam format 24 jam (0-23),
2. `Day of Week` berisi informasi hari dalam seminggu (0-6), di mana 0 adalah Ahad, 1 adalah Senin, dan seterusnya.

Kemudian melakukan deskripsi fitur numerik untuk melihat informasi pada fitur
"""

df.describe()

"""### Univariate Analysis

Analisis univariate adalah teknik analisis data yang berfokus pada satu variabel saja untuk memahami distribusi atau karakteristiknya. Dalam analisis ini, kita mengeksplorasi setiap fitur atau variabel secara individu
"""

# Melihat distrbusi pada fitur numerik
plt.figure(figsize=(12, 6))
df.hist(bins=30, figsize=(15, 10))
plt.suptitle('Histogram dari Fitur Numerik')
plt.show()

# Mengatur ukuran
plt.figure(figsize=(10, 6))

# Mengonversi kolom Date ke format datetime jika belum
df['Date'] = pd.to_datetime(df['Date'])

# Membuat histogram untuk kolom Date
plt.hist(df['Date'], bins=30, color='blue', edgecolor='black')
plt.title('Histogram Date')
plt.xlabel('Tanggal')
plt.ylabel('Frekuensi')
plt.show()

# Distrusi untuk data kategorikal `Primary Type`
plt.figure(figsize=(12, 6))
sns.countplot(y=df['Primary Type'], order=df['Primary Type'].value_counts().index)
plt.title('Distribusi Frekuensi Jenis Kejahatan (Primary Type)')
plt.show()

# Membatasi kategori yang akan ditampilkan (hanya 20 kategori teratas)
top_categories = df['Location Description'].value_counts().nlargest(20).index

# Membuat plot
plt.figure(figsize=(12, 6))
sns.countplot(y=df[df['Location Description'].isin(top_categories)]['Location Description'],
              order=top_categories)
plt.title('Distribusi Frekuensi Tempat Kejadian (Location Description)')
plt.show()

# Distribusi untuk data kategorikal biner (contoh: Arrest)
plt.figure(figsize=(6, 4))
sns.countplot(x=df['Arrest'])
plt.title('Distribusi Kejadian dengan Penangkapan (Arrest)')
plt.show()

# Distribusi untuk data kategorikal biner (contoh: Arrest)
plt.figure(figsize=(6, 4))
sns.countplot(x=df['Domestic'])
plt.title('Distribusi jenis kejahatan Domestic')
plt.show()

"""### Multivariate

Analisis multivariate adalah teknik analisis data yang melibatkan beberapa variabel sekaligus untuk memahami hubungan atau interaksi antar variabel tersebut.
"""

# Membuat plot untuk semua fitur numerik
numerical_features = ['Beat', 'District', 'Latitude', 'Longitude', 'Community Area', 'Hour', 'Day of Week']
sns.pairplot(df[numerical_features])
plt.suptitle('Multivariate Analysis antar Fitur Numerik', y=1.02)
plt.show()

# Distribusi jenis kejahatan berdasarkan waktu (jam)
plt.figure(figsize=(12, 6))
sns.scatterplot(x='Hour', y='Primary Type', data=df)
plt.title('Distribusi Jenis Kejahatan berdasarkan Jam Kejadian')
plt.show()

# Plot Rata-rata jam kejadian per jenis kejahatan
plt.figure(figsize=(12, 6))
sns.barplot(x='Primary Type', y='Hour', data=df)
plt.title('Rata-rata Jam Kejadian per Jenis Kejahatan')
plt.xticks(rotation=90)
plt.show()

# Day of Week vs Primary Type
plt.figure(figsize=(12, 8))
sns.scatterplot(x=df['Day of Week'], y=df['Primary Type'])
plt.title('Distribusi Jenis Kejahatan Berdasarkan Hari dalam Seminggu')
plt.show()

# Arrest, Domestic, Hour, and Primary Type
plt.figure(figsize=(12, 8))
sns.scatterplot(x=df['Hour'], y=df['Primary Type'], hue=df['Arrest'], style=df['Domestic'])
plt.title('Jenis Kejahatan Berdasarkan Jam, Arrest, dan Domestic')
plt.show()

# Distribusi Lokasi Kejahatan Berdasarkan Community Area
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='Longitude', y='Latitude', hue='Community Area', palette='Set1')

# Memindahkan legend ke bawah plot
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=10, title="Community Area")
plt.title('Distribusi Lokasi Kejahatan Berdasarkan Community Area')
plt.tight_layout()
plt.show()

# Distribusi Jenis Kejahatan Berdasarkan Lokasi
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='Longitude', y='Latitude', hue='Primary Type', palette='Set2')
plt.title('Distribusi Jenis Kejahatan Berdasarkan Lokasi')

# Memindahkan legend ke bawah plot
plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)
plt.tight_layout()
plt.show()

# Distribusi Lokasi Kejahatan Berdasarkan Waktu (Hour)
plt.figure(figsize=(10, 8))
sns.scatterplot(data=df, x='Longitude', y='Latitude', hue='Hour', palette='cool', alpha=0.6)
plt.title('Distribusi Lokasi Kejahatan Berdasarkan Waktu (Hour)')
plt.show()

# Distribusi Kejahatan Berdasarkan Lokasi (Latitude dan Longitude)
plt.figure(figsize=(10, 8))
sns.scatterplot(x=df['Longitude'], y=df['Latitude'], hue=df['Community Area'], palette='coolwarm', legend=False)
plt.title('Distribusi Kejahatan Berdasarkan Lokasi (Latitude dan Longitude)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

# Plot untuk Hour dan Day of Week
plt.figure(figsize=(10, 6))
sns.boxplot(x='Day of Week', y='Hour', data=df)
plt.title('Distribusi Waktu Kejahatan Berdasarkan Hari dalam Minggu')
plt.xlabel('Hari dalam Minggu (0=Senin, 6=Minggu)')
plt.ylabel('Jam Kejadian')
plt.show()

# Membuat DataFrame baru hanya dengan fitur numerik dari dataset
df_numeric = df[['Beat', 'District', 'Latitude', 'Longitude', 'Community Area', 'Hour', 'Day of Week']]

# Menghitung korelasi antar fitur numerik
correlation_matrix = df_numeric.corr()

# Membuat heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Matriks Korelasi untuk Fitur Numerik')
plt.show()

"""# Data Preperation

Pada Data Preperation ini dilakukan pembersihan data diantaranya, menghapus *missing values*, duplikasi, dan *outliers*. Setelah dibersihkan, data akan di *encoding* terlebih dahulu sebelum melewati proses pembagian data, kemudian data yang sudah dibagi akan di*scaling*. Data yang sudah di*scaling* akan dimasukkan kedalam model untuk dilatih dan diuji / prediksi.

## Menangani Missing Values

Melakukan penghapusan baris pada missing values, dengan harapan setiap baris pada data tidak terdapat missing values.
"""

# Memeriksa missing values
print(df.isnull().sum())

# Menghapus baris yang memiliki missing values
df = df.dropna()

# Menampilkan kolom yang terdapat missing values
print(df.isnull().sum())

# Melihat kembali informasi pada dataset
df.info()

"""Setelah penghapusan *missing values*, jumlah data menjadi `930769`.

## Menghapus Duplikasi

Proses ini akan dilakukan penghapusan data duplikat yang identik, dengan harapan setelah data duplikat dihapus, data yang ada itu bersifat *unique*.
"""

# Menyimpan jumlah baris sebelum penghapusan duplikasi
before = df.shape[0]
print(f"Jumlah baris sebelum menghapus duplicates: {before}")

# Memeriksa jumlah total baris duplikat
total_duplicates = df.duplicated().sum()
print(f"Jumlah total baris duplikat: {total_duplicates}")

# Menghapus baris yang terduplikasi secara identik
df = df.drop_duplicates()

# Menampilkan jumlah baris sebelum dan setelah menghapus duplikasi
after = df.shape[0]
print(f"Jumlah baris setelah menghapus duplicates: {after}")

df.info()

"""Setelah penghapusan data duplikat, jumlah data menjadi `929122`.

## Menghapus Outliers

Penghapusan *outliers* dilakukan untuk meningkatkan kualitas data dengan menghilangkan nilai-nilai yang jauh berbeda dari distribusi umum data. *Outliers* sering kali mengganggu analisis atau pemodelan.
"""

# Menentukan kolom numerik
numerical_columns = ['Beat', 'District', 'Latitude', 'Longitude', 'Community Area', 'Hour', 'Day of Week']

# Mengatur ukuran figure
plt.figure(figsize=(15, 10))

# Membuat box plot untuk setiap kolom numerik
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)  # Mengatur jumlah subplot dalam grid
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot {col}')

# Mengatur layout agar plot tidak tumpang tindih
plt.tight_layout()
plt.show()

"""melihat lebih dekat pada 2 box longitude dan latitude"""

# Menentukan kolom numerik
numerical_columns = ['Latitude', 'Longitude']

# Mengatur ukuran figure
plt.figure(figsize=(15, 10))

# Membuat box plot untuk setiap kolom numerik
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)  # Mengatur jumlah subplot dalam grid
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot {col}')

# Mengatur layout agar plot tidak tumpang tindih
plt.tight_layout()
plt.show()

"""melihat adanya outliers di kolom Longitude dan Latitude, maka dari itu akan dilakukan penghapusan outliers dengan metode IQR. Formula metode IQR dapat digambarkan sebagai berikut:<br>
   $IQR=Q_3-Q_1$
   
   Kemudian membuat batas bawah dan batas atas untuk mencakup *outliers* dengan menggunakan,
   
   $BatasBawah=Q_1-1.5*IQR$
   
   $BatasAtas=Q_3-1.5*IQR$
"""

print(f"Jumlah baris sebelum penghapusan outliers: {df.shape[0]}")

# Menghitung Q1 dan Q3 untuk Longitude
Q1_longitude = df['Longitude'].quantile(0.25)
Q3_longitude = df['Longitude'].quantile(0.75)
IQR_longitude = Q3_longitude - Q1_longitude

# Tentukan batas atas dan bawah untuk outliers Longitude
lower_bound_longitude = Q1_longitude - 1.5 * IQR_longitude
upper_bound_longitude = Q3_longitude + 1.5 * IQR_longitude

# Filter DataFrame untuk menghapus outliers Longitude
df_outliers_longitude = df[(df['Longitude'] >= lower_bound_longitude) & (df['Longitude'] <= upper_bound_longitude)]

# Menghitung Q1 dan Q3 untuk Latitude
Q1_latitude = df['Latitude'].quantile(0.25)
Q3_latitude = df['Latitude'].quantile(0.75)
IQR_latitude = Q3_latitude - Q1_latitude

# Tentukan batas atas dan bawah untuk outliers Latitude
lower_bound_latitude = Q1_latitude - 1.5 * IQR_latitude
upper_bound_latitude = Q3_latitude + 1.5 * IQR_latitude

# Filter DataFrame untuk menghapus outliers Latitude
df_outliers_latitude = df[(df['Latitude'] >= lower_bound_latitude) & (df['Latitude'] <= upper_bound_latitude)]

# Gabungkan hasil penghapusan outliers dari Longitude dan Latitude
df = pd.merge(df_outliers_longitude, df_outliers_latitude, how='inner')

# Menampilkan jumlah baris setelah penghapusan outliers
print(f"\nJumlah baris setelah penghapusan outliers: {df.shape[0]}")

"""Setelah dilakukan penghapusan *outliers*, dilakukan pengecekan kembali untuk melihat *outliers*, awalnya berjumlah `929122` baris kemudian setelah dihapus, jumlah baris menurun menjadi `924193` baris. Dan dilakukan visualisasi kembali guna mengecek apakah terdapat *outliers*"""

# Menentukan kolom numerik
numerical_columns = ['Latitude', 'Longitude']

# Mengatur ukuran figure
plt.figure(figsize=(15, 10))

# Membuat box plot untuk setiap kolom numerik
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)  # Mengatur jumlah subplot dalam grid
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot {col}')

# Mengatur layout agar plot tidak tumpang tindih
plt.tight_layout()
plt.show()

"""Visualisasi diatas menunjukkan bahwa masih terdapat *outliers* pada fitur `Longitude` tetapi masih dalam batas aman.

## Drop Tabel
"""

#del df

#import pandas as pd
#df = pd.read_csv('/content/drive/MyDrive/Dicoding/Machine Learning Terapan/P1/cleaned_crime_without_hour_n_day.csv')

# URL langsung ke file
url = 'https://drive.google.com/uc?id=1nwg0xL-Ef8nbjWJoLKVDJg3i-tH6QvG5'

# Membaca CSV
df = pd.read_csv(url)

# Menghapus kolom 'Date'
df = df.drop(columns=['Date'])

df.head()

df.head()

# Menghapus kolom 'Date', 'Hour', dan 'Day of Week' dari DataFrame
df = df.drop(columns=['Date', 'Hour', 'Day of Week'])

# Menampilkan 5 baris pertama untuk memastikan kolom telah dihapus
df.head()

"""## Perubahan Fitur"""

# Menghitung distribusi untuk setiap kategori di Community Area
value_counts = df['Community Area'].value_counts()

# Membuat mapping berdasarkan kategori
category_map = {}
for community_area, count in value_counts.items():
    if count > 30000:
        category_map[community_area] = 'Sering Terjadi'
    elif count > 5000:
        category_map[community_area] = 'Jarang Terjadi'
    else:
        category_map[community_area] = 'Sangat Jarang Terjadi'

# Menggunakan mapping untuk mengelompokkan kategori
df['Community Area'] = df['Community Area'].map(category_map)

# Mengubah kategori menjadi numerik di kolom yang sama
group_mapping = {'Sering Terjadi': 0, 'Jarang Terjadi': 1, 'Sangat Jarang Terjadi': 2}
df['Community Area'] = df['Community Area'].map(group_mapping)

df.head()

# Menghitung 3 kelas yang baru diapply
df['Community Area'].value_counts()

"""## Encoding

Pada proses ini akan dilakukan perubahan pada fitur kategori seperti `Primary Type`, dan `Location Description` diubah menjadi fitur numerik, fitur lain seperti `Arrest` dan `Domestic` dari kategori `boolean` berupa teks True / False akan diubah menjadi numerik (0 atau 1). Proses Encoding ini menggunakan LabelEncoder dari *library* SKLearn.
"""

# del df_encoded

# Menginisialisasi LabelEncoder
le = LabelEncoder()

df_encoded = df

# Menggunakan LabelEncoder untuk fitur 'Primary Type dan Location Description'
df_encoded['Primary Type'] = le.fit_transform(df['Primary Type'])
df_encoded['Location Description'] = le.fit_transform(df['Location Description'])

# Mengubah kolom boolean menjadi 0 atau 1
df_encoded['Arrest'] = df_encoded['Arrest'].astype(int)
df_encoded['Domestic'] = df_encoded['Domestic'].astype(int)

# Melihat hasil encoding
df_encoded.head()

"""## Split Data

Sebelum data di bagi, dilakukan pemisahan fitur dan target dari dataset. Semua kolom kecuali `Community Area` dipilih sebagai fitur, sementara kolom `Community Area` dijadikan target yang ingin diprediksi atau klasifikasi. setelah pemisahan fitur dan target, dilakukan pembagian data pada dataset. Rasio pembagian data ini adalah 70:30, 70% untuk data latih, dan 30% untuk data uji. Pada tahapan ini akan memisahkan fitur dan target dalam dataset.
"""

# Memisahkan fitur dan target
X = df_encoded.drop(columns=['Community Area'])  # Semua kolom kecuali 'Community Area' sebagai fitur
y = df_encoded['Community Area']  # Kolom 'Community Area' sebagai target

# Membagi data menjadi 90% untuk training dan 10% untuk testing, tanpa stratifikasi
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Menampilkan ukuran data hasil split
print(f"Ukuran data training: {X_train.shape}, target training: {y_train.shape}")
print(f"Ukuran data testing: {X_test.shape}, target testing: {y_test.shape}")

"""## Scaling

Setelah data dibagi, langkah selanjutnya adalah melakukan proses scaling pada fitur-fitur dalam data yang telah dibagi. Scaling ini bertujuan untuk menyelaraskan rentang nilai setiap fitur, sehingga tidak ada fitur yang mendominasi atau mempengaruhi model secara berlebihan. Dengan menggunakan metode MinMax Scaling dari *library* SKLearn, setiap nilai fitur akan dipetakan dalam rentang 0 hingga 1. Scaling dilakukan pada data latih dan data uji.
"""

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Melakukan scaling pada data latih
X_train_scaled = scaler.fit_transform(X_train)

# Melakukan scaling pada data uji
X_test_scaled = scaler.transform(X_test)

# Mengecek apakah nilai di antara 0 dan 1
print(f"Min data training: {X_train_scaled.min()}")
print(f"Max data training: {X_train_scaled.max()}")
print(f"Min data testing: {X_test_scaled.min()}")
print(f"Max data testing: {X_test_scaled.max()}")

"""# Modeling

Pada tahapan akan menjawab **Problem Statement** dalam membangun model. Setelah data dibagi dan di*scaling*, dilakukan pembangunan model, model yang dipilih ialah KNN (K-Nearest Neighbor) dan Naive Bayes untuk dibandingkan performanya, setiap model akan dilatih menggunakan data latih dan setelah model dibangun akan dilakukan pengujian dengan data uji untuk melihat performa kemampuan model dalam klasifikasi.
"""

# Inisialisasi model Naive Bayes dan KNN
nb_model = GaussianNB()
knn_model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')

# Melatih model Naive Bayes pada data latih
nb_model.fit(X_train_scaled, y_train)

# Melatih model KNN pada data latih
knn_model.fit(X_train_scaled, y_train)

"""Setelah setiap model dilatih, dilakukan prediksi menggunakan data uji, dan nilai hasil prediksi akan disimpan untuk dievaluasi."""

# Prediksi dan akurasi untuk Naive Bayes
nb_train_pred = nb_model.predict(X_train_scaled)
nb_test_pred = nb_model.predict(X_test_scaled)
nb_train_accuracy = accuracy_score(y_train, nb_train_pred)
nb_test_accuracy = accuracy_score(y_test, nb_test_pred)

# Prediksi dan akurasi untuk KNN
knn_train_pred = knn_model.predict(X_train_scaled)
knn_test_pred = knn_model.predict(X_test_scaled)
knn_train_accuracy = accuracy_score(y_train, knn_train_pred)
knn_test_accuracy = accuracy_score(y_test, knn_test_pred)

# Menyimpan hasil akurasi dalam variabel untuk visualisasi nanti
train_accuracies = [nb_train_accuracy, knn_train_accuracy]
test_accuracies = [nb_test_accuracy, knn_test_accuracy]

"""# Evaluasi

Setelah model diprediksi menggunakan data uji, dilakukan evaluasi guna melihat performa model dalam mengklasifikasikan. Evaluasi ini menggunakan *Confusion Matrix* sebagai tolak ukur hasil dari prediksi.
"""

# Confusion matrix untuk data latih Naive Bayes
nb_train_cm = confusion_matrix(y_train, nb_train_pred)
nb_train_cm_df = pd.DataFrame(nb_train_cm, index=['Actual 0', 'Actual 1', 'Actual 2'],
                              columns=['Predicted 0', 'Predicted 1', 'Predicted 2'])

# Confusion matrix untuk data uji Naive Bayes
nb_test_cm = confusion_matrix(y_test, nb_test_pred)
nb_test_cm_df = pd.DataFrame(nb_test_cm, index=['Actual 0', 'Actual 1', 'Actual 2'],
                             columns=['Predicted 0', 'Predicted 1', 'Predicted 2'])

# Confusion matrix untuk data latih KNN
knn_train_cm = confusion_matrix(y_train, knn_train_pred)
knn_train_cm_df = pd.DataFrame(knn_train_cm, index=['Actual 0', 'Actual 1', 'Actual 2'],
                               columns=['Predicted 0', 'Predicted 1', 'Predicted 2'])

# Confusion matrix untuk data uji KNN
knn_test_cm = confusion_matrix(y_test, knn_test_pred)
knn_test_cm_df = pd.DataFrame(knn_test_cm, index=['Actual 0', 'Actual 1', 'Actual 2'],
                              columns=['Predicted 0', 'Predicted 1', 'Predicted 2'])

# Menampilkan confusion matrix dalam bentuk DataFrame
print("Confusion Matrix Naive Bayes - Data Latih:\n", nb_train_cm_df)
print("\nConfusion Matrix Naive Bayes - Data Uji:\n", nb_test_cm_df)
print("\nConfusion Matrix KNN - Data Latih:\n", knn_train_cm_df)
print("\nConfusion Matrix KNN - Data Uji:\n", knn_test_cm_df)

"""Setelah nilai dari *Confusion Matrix* didapat, dilakukan perhitungan metrik akurasi sebagai acuan performa model dalam klasifikasi."""

# Membuat DataFrame dengan akurasi model
model_accuracies = {
    'Model': ['Naive Bayes', 'KNN'],
    'Data Latih': [nb_train_accuracy, nb_test_accuracy],
    'Data Uji': [knn_train_accuracy, knn_test_accuracy]
}

# Membuat DataFrame
accuracy_df = pd.DataFrame(model_accuracies)

# Menampilkan DataFrame
print(accuracy_df)

"""Melihat nilai akurasi dalam bentuk visualisasi"""

# Nama model untuk visualisasi
models = ['Naive Bayes', 'KNN']
x = range(len(models))

plt.figure(figsize=(8, 5))
plt.bar(x, train_accuracies, width=0.4, label='Akurasi data latih', color='#87CEEB', align='center')
plt.bar([p + 0.4 for p in x], test_accuracies, width=0.4, label='Akurasi data uji', color='#FFB6A9', align='center')
plt.xlabel('Model', fontsize=10)
plt.ylabel('Akurasi', fontsize=10)
plt.title('Akurasi Model Naive Bayes dan KNN', fontsize=12)
plt.xticks([p + 0.2 for p in x], models, fontsize=10)
plt.ylim(0, 1)
plt.legend(fontsize=10)

# Menampilkan nilai akurasi pada setiap bar
for i, v in enumerate(train_accuracies):
    plt.text(i, v + 0.02, f"{v:.4f}", ha='center', fontweight='bold')
for i, v in enumerate(test_accuracies):
    plt.text(i + 0.4, v + 0.02, f"{v:.4f}", ha='center', fontweight='bold')

# Menampilkan Grafik
plt.show()

"""Dari hasil perhitungan akurasi model, terlihat bahwa model Naive Bayes dan KNN menunjukkan performa yang berbeda pada data latih dan data uji. Model Naive Bayes memiliki akurasi sebesar 0.7166 pada data latih dan 0.9865 pada data uji. Sementara itu, model KNN menunjukkan akurasi 0.7156 pada data latih dan 0.9774 pada data uji.

Hasil ini dapat menjawab **Problem Statement** yang mana model Naive Bayes dan KNN sama-sama menunjukkan akurasi tinggi pada data uji, yang mengindikasikan bahwa keduanya berhasil menangkap pola-pola penting dalam data, meskipun terdapat sedikit perbedaan pada akurasi data latih. Akurasi yang tinggi pada data uji ini menunjukkan bahwa model mampu membedakan area dengan kategori yang berbeda misalnya, area dengan kategori "Jarang Terjadi," "Sering Terjadi," dan "Sangat Jarang Terjadi" dengan baik, berdasarkan pola kejadian di area tersebut.

Dengan hasil ini dapat menyimpulkan bahwa baik Naive Bayes maupun KNN adalah model yang cocok untuk membantu prediksi area dengan tingkat kejadian tertentu dalam area yang diklasifikasikan.
"""